# DualAnoDiff è¨“ç·´æŒ‡å—

æœ¬æŒ‡å—è¨˜éŒ„äº†åœ¨æ–°ç’°å¢ƒä¸­è¨­ç½®å’Œé‹è¡Œ DualAnoDiff è¨“ç·´çš„å®Œæ•´æ­¥é©Ÿï¼ŒåŒ…æ‹¬æ‰€æœ‰é‡åˆ°çš„å•é¡Œå’Œè§£æ±ºæ–¹æ¡ˆã€‚

## ç›®éŒ„

- [ç³»çµ±éœ€æ±‚](#ç³»çµ±éœ€æ±‚)
- [ç’°å¢ƒè¨­ç½®](#ç’°å¢ƒè¨­ç½®)
- [ä»£ç¢¼ä¿®æ”¹](#ä»£ç¢¼ä¿®æ”¹)
- [è¨“ç·´åŸ·è¡Œ](#è¨“ç·´åŸ·è¡Œ)
- [å•é¡Œæ’æŸ¥](#å•é¡Œæ’æŸ¥)
- [ç›£æ§è¨“ç·´](#ç›£æ§è¨“ç·´)

---

## ç³»çµ±éœ€æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA GPU with >= 20GB VRAM (æ¸¬è©¦ä½¿ç”¨ RTX A4500)
- **RAM**: >= 32GB
- **å­˜å„²**: >= 100GB å¯ç”¨ç©ºé–“

### è»Ÿä»¶è¦æ±‚
- **OS**: Linux (æ¸¬è©¦æ–¼ Ubuntu with kernel 6.8.0-64)
- **CUDA**: 11.8
- **Python**: 3.10 (ä¸è¦ä½¿ç”¨ 3.13ï¼Œå­˜åœ¨å…¼å®¹æ€§å•é¡Œ)
- **Conda**: Miniconda æˆ– Anaconda

---

## ç’°å¢ƒè¨­ç½®

### 1. å‰µå»º Conda ç’°å¢ƒ

```bash
# é€²å…¥é …ç›®ç›®éŒ„
cd /path/to/DualAnoDiff

# å‰µå»º Python 3.10 ç’°å¢ƒï¼ˆé‡è¦ï¼šä¸è¦ä½¿ç”¨ 3.13ï¼‰
conda create -p ./.env python=3.10 -y
conda activate ./.env
```

### 2. ä¿®å¾© requirements.txt

åŸå§‹çš„ `requirements.txt` å­˜åœ¨æ ¼å¼éŒ¯èª¤ï¼Œéœ€è¦ä¿®å¾©ï¼š

```bash
# ç·¨è¼¯ requirements.txtï¼Œä¿®å¾©ä»¥ä¸‹å…©è¡Œï¼š
# ç¬¬ 9 è¡Œï¼šopencv-python-headless==4.7.0.72  (åŸç‚º .7.0.72)
# ç¬¬ 18 è¡Œï¼štensorboard==2.15.0  (åŸç‚º .15.0)
```

ä¿®å¾©å¾Œçš„ requirements.txtï¼š
```
accelerate==0.24.1
clip
Cython==0.29.35
matplotlib==3.8.0
numpy==1.24.3
open-clip-torch==2.23.0
opencv-python==4.7.0.72
opencv-python-headless==4.7.0.72  # å·²ä¿®å¾©
opencv-python-headless==4.7.0.72
pandas==2.0.3
Pillow==9.4.0
pytorch-lightning==1.5.0
PyYAML==6.0
scikit-image==0.22.0
scikit-learn==1.3.2
scipy==1.10.1
setuptools==65.6.3
tensorboard==2.15.0  # å·²ä¿®å¾©
timm==0.4.12
torch==2.0.1+cu118
torchaudio==2.0.2+cu118
torchmetrics==0.6.0
torchvision==0.15.2+cu118
transformers==4.30.2
```

### 3. å®‰è£ä¾è³´

**é‡è¦ï¼šå¿…é ˆæŒ‰ä»¥ä¸‹é †åºå®‰è£ï¼Œä¸¦é€²è¡Œ PyTorch å‡ç´š**

```bash
# Step 1: å®‰è£åŸºç¤ä¾è³´
pip install -r requirements.txt --index-url https://download.pytorch.org/whl/cu118

# Step 2: å®‰è£ç¼ºå¤±çš„ä¾è³´
pip install einops
pip install "huggingface_hub<0.20"  # ç‰ˆæœ¬ 0.19.4
pip install ipdb
pip install diffusers

# Step 3: é—œéµæ­¥é©Ÿ - å‡ç´š PyTorch åˆ° 2.2.0
# é€™æ˜¯è§£æ±ºæ··åˆç²¾åº¦è¨“ç·´ dtype éŒ¯èª¤çš„é—œéµ
pip install --upgrade torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118

# Step 4: å‡ç´š xformers ä»¥åŒ¹é… PyTorch 2.2
pip install --upgrade xformers==0.0.24
```

### 4. é…ç½® Accelerate

```bash
# å‰µå»º accelerate é…ç½®ç›®éŒ„
mkdir -p ~/.cache/huggingface/accelerate

# å‰µå»ºé…ç½®æ–‡ä»¶
cat > ~/.cache/huggingface/accelerate/default_config.yaml << 'EOF'
compute_environment: LOCAL_MACHINE
deepspeed_config: {}
distributed_type: 'NO'
downcast_bf16: 'no'
fsdp_config: {}
machine_rank: 0
main_process_ip: null
main_process_port: null
main_training_function: main
megatron_lm_config: {}
mixed_precision: 'no'  # é‡è¦ï¼šä½¿ç”¨ fp32 è¨“ç·´
num_machines: 1
num_processes: 1
rdzv_backend: static
same_network: true
use_cpu: false
EOF
```

### 5. æº–å‚™æ•¸æ“šé›†

```bash
# ä¸‹è¼‰ä¸¦è§£å£“ MVTec AD æ•¸æ“šé›†åˆ°ï¼š
# /path/to/mvtec_anomaly_detection/
#
# ç›®éŒ„çµæ§‹æ‡‰ç‚ºï¼š
# mvtec_anomaly_detection/
# â”œâ”€â”€ bottle/
# â”œâ”€â”€ cable/
# â”œâ”€â”€ toothbrush/
# â”‚   â”œâ”€â”€ train/
# â”‚   â”‚   â””â”€â”€ good/
# â”‚   â””â”€â”€ test/
# â”‚       â”œâ”€â”€ good/
# â”‚       â””â”€â”€ defective/
# â””â”€â”€ ...
```

---

## ä»£ç¢¼ä¿®æ”¹

### ä¿®æ”¹ 1: æ›´æ–°è·¯å¾‘é…ç½®

**æ–‡ä»¶**: `run_mvtec_split_background_control_scale.py`

```python
# ç¬¬ 9-11 è¡Œï¼šæ›´æ–°ç‚ºçµ•å°è·¯å¾‘
export MODEL_NAME="/path/to/stable-diffusion-v1-5"
export INSTANCE_DIR="/path/to/mvtec_anomaly_detection/{name}/test/{anomaly}"
export OUTPUT_DIR="all_generate/{name}/{anomaly}"

# ç¬¬ 53 è¡Œï¼šæ›´æ–° MVTec æ•¸æ“šé›†è·¯å¾‘
for anomaly in os.listdir(os.path.join('/path/to/mvtec_anomaly_detection',name,'test')):
```

### ä¿®æ”¹ 2: ä¿®å¾©æ··åˆç²¾åº¦å•é¡Œï¼ˆé—œéµä¿®æ”¹ï¼‰

**æ–‡ä»¶**: `train_dreambooth_lora_single_background.py`

åœ¨ç¬¬ 1060 è¡Œå¾Œï¼ˆ`unet.set_attn_processor(unet_lora_attn_procs)` ä¹‹å¾Œï¼‰æ·»åŠ ä»¥ä¸‹ä»£ç¢¼ï¼š

```python
unet.set_attn_processor(unet_lora_attn_procs)
unet_lora_layers = AttnProcsLayers(unet.attn_processors)

# ========== æ·»åŠ ä»¥ä¸‹ä»£ç¢¼ ==========
# Move temporal layers in attention processors to weight_dtype
for attn_processor in unet.attn_processors.values():
    if hasattr(attn_processor, 'chained_proc'):
        # For ReferenceOnlyAttnProc wrappers
        if hasattr(attn_processor.chained_proc, 'temporal_n'):
            attn_processor.chained_proc.temporal_n.to(dtype=weight_dtype)
            attn_processor.chained_proc.temporal_i.to(dtype=weight_dtype)
            attn_processor.chained_proc.temporal_q.to(dtype=weight_dtype)
            attn_processor.chained_proc.temporal_k.to(dtype=weight_dtype)
            attn_processor.chained_proc.temporal_v.to(dtype=weight_dtype)
            attn_processor.chained_proc.temporal_o.to(dtype=weight_dtype)
            attn_processor.chained_proc.temporal_condition_mlp.to(dtype=weight_dtype)
    elif hasattr(attn_processor, 'temporal_n'):
        # For direct LoRAAttnProcessor2_0 instances
        attn_processor.temporal_n.to(dtype=weight_dtype)
        attn_processor.temporal_i.to(dtype=weight_dtype)
        attn_processor.temporal_q.to(dtype=weight_dtype)
        attn_processor.temporal_k.to(dtype=weight_dtype)
        attn_processor.temporal_v.to(dtype=weight_dtype)
        attn_processor.temporal_o.to(dtype=weight_dtype)
        attn_processor.temporal_condition_mlp.to(dtype=weight_dtype)
# ========== çµæŸæ·»åŠ  ==========

# The text encoder comes from ğŸ¤— transformers, so we cannot directly modify it.
```

### ä¿®æ”¹ 3: ç°¡åŒ– LoRA ä»£ç¢¼

**æ–‡ä»¶**: `diffusers/models/attention_processor.py`

æ‰¾åˆ° `LoRAAttnProcessor2_0.__call__` æ–¹æ³•ä¸­çš„ LoRA è¨ˆç®—éƒ¨åˆ†ï¼ˆç´„ 1370-1413 è¡Œï¼‰ï¼Œæ›¿æ›ç‚ºï¼š

```python
# åŸå§‹ä»£ç¢¼æœ‰è¤‡é›œçš„ dtype è½‰æ›å’Œ autocast è™•ç†
# æ›¿æ›ç‚ºç°¡åŒ–ç‰ˆæœ¬ï¼š

# LoRA query
query = attn.to_q(hidden_states) + scale * self.to_q_lora[i](hidden_states)

# self:
if 'condition_encoder_state' in kwargs and i == 0:
    condition_encoder_state = kwargs['condition_encoder_state']
    # Blend condition encoder state with encoder hidden states
    condition_encoder_state_processed = self.temporal_condition_mlp(condition_encoder_state)
    blended = (1 - self.temporal_a) * encoder_hidden_states + self.temporal_a * condition_encoder_state_processed
    encoder_hidden_states_for_kv = blended
else:
    encoder_hidden_states_for_kv = encoder_hidden_states

# LoRA key/value
key_input = encoder_hidden_states_for_kv if encoder_hidden_states_for_kv is not None else hidden_states
value_input = encoder_hidden_states_for_kv if encoder_hidden_states_for_kv is not None else hidden_states
key = attn.to_k(key_input) + scale * self.to_k_lora[i](key_input)
value = attn.to_v(value_input) + scale * self.to_v_lora[i](value_input)
```

åŒæ™‚åœ¨ç´„ 1452 è¡Œï¼Œå°‡ï¼š
```python
x = hidden_states.half()  # Force fp16 for temporal processing in mixed precision
```
æ”¹ç‚ºï¼š
```python
x = hidden_states  # Keep original dtype (fp32 when mixed precision disabled, fp16 when enabled)
```

---

## è¨“ç·´åŸ·è¡Œ

### å‰µå»ºè¨“ç·´è…³æœ¬

```bash
cd /path/to/DualAnoDiff/bcm-dual-interrelated_diff

cat > start_training.sh << 'EOF'
#!/bin/bash
cd /path/to/DualAnoDiff/bcm-dual-interrelated_diff
source /path/to/miniconda3/etc/profile.d/conda.sh
conda activate /path/to/DualAnoDiff/.env

CUDA_VISIBLE_DEVICES=0 accelerate launch train_dreambooth_lora_single_background.py \
  --pretrained_model_name_or_path="/path/to/stable-diffusion-v1-5" \
  --instance_data_dir="/path/to/mvtec_anomaly_detection/toothbrush/test/defective" \
  --output_dir="all_generate/toothbrush/defective" \
  --instance_prompt="a srw" \
  --resolution=512 \
  --train_batch_size=2 \
  --gradient_accumulation_steps=1 \
  --learning_rate=2e-5 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --max_train_steps=5000 \
  --rank=32 \
  --train_text_encoder
EOF

chmod +x start_training.sh
```

### å•Ÿå‹•è¨“ç·´

```bash
# ä½¿ç”¨ nohup åœ¨å¾Œå°é‹è¡Œ
nohup bash start_training.sh > nohup_train_toothbrush_defective.log 2>&1 &

# è¨˜ä¸‹ PID
echo $!
```

### è¨“ç·´åƒæ•¸èªªæ˜

- `--train_batch_size=2`: æ¯å€‹ GPU çš„ batch sizeï¼ˆ20GB VRAM å¯ç”¨ 2ï¼‰
- `--max_train_steps=5000`: ç¸½è¨“ç·´æ­¥æ•¸
- `--rank=32`: LoRA ç§©
- `--train_text_encoder`: åŒæ™‚è¨“ç·´æ–‡æœ¬ç·¨ç¢¼å™¨
- `--resolution=512`: åœ–åƒåˆ†è¾¨ç‡

**é è¨ˆè¨“ç·´æ™‚é–“**: ç´„ 4-5 å°æ™‚ï¼ˆRTX A4500, batch_size=2ï¼‰

---

## å•é¡Œæ’æŸ¥

### å•é¡Œ 1: Python 3.13 å…¼å®¹æ€§éŒ¯èª¤

**éŒ¯èª¤**:
```
AttributeError: module 'pkgutil' has no attribute 'ImpImporter'
```

**è§£æ±ºæ–¹æ¡ˆ**: ä½¿ç”¨ Python 3.10
```bash
conda create -p ./.env python=3.10 -y
```

### å•é¡Œ 2: æ··åˆç²¾åº¦ dtype ä¸åŒ¹é…

**éŒ¯èª¤**:
```
RuntimeError: mat1 and mat2 must have the same dtype, but got Half and Float
RuntimeError: expected scalar type Half but found Float
```

**åŸå› **: PyTorch 2.0.x åœ¨è‡ªå®šç¾© attention processors ä¸­çš„æ··åˆç²¾åº¦è¨“ç·´å­˜åœ¨ bug

**è§£æ±ºæ–¹æ¡ˆ**:
1. å‡ç´š PyTorch åˆ° 2.2.0
2. ä½¿ç”¨ fp32 è¨“ç·´ï¼ˆ`mixed_precision: 'no'`ï¼‰
3. æ·»åŠ ä»£ç¢¼å°‡ temporal å±¤ç§»å‹•åˆ°æ­£ç¢ºçš„ dtypeï¼ˆè¦‹ä»£ç¢¼ä¿®æ”¹ 2ï¼‰

### å•é¡Œ 3: xformers ç‰ˆæœ¬ä¸åŒ¹é…

**éŒ¯èª¤**:
```
xformers 0.0.17 requires torch==2.0.0, but you have torch 2.2.0
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
pip install --upgrade xformers==0.0.24
```

### å•é¡Œ 4: huggingface_hub å…¼å®¹æ€§

**éŒ¯èª¤**:
```
ImportError: cannot import name 'cached_download' from 'huggingface_hub'
```

**è§£æ±ºæ–¹æ¡ˆ**:
```bash
pip install "huggingface_hub<0.20"
```

### å•é¡Œ 5: nohup æ—¥èªŒç‚ºç©º

**åŸå› **: ä½¿ç”¨ `conda run` åŒ…è£¹å°è‡´è¼¸å‡ºé‡å®šå‘å¤±æ•—

**è§£æ±ºæ–¹æ¡ˆ**: å‰µå»ºç¨ç«‹çš„ bash è…³æœ¬ä¸¦æ¿€æ´»ç’°å¢ƒ
```bash
# åœ¨è…³æœ¬ä¸­ä½¿ç”¨ï¼š
source /path/to/conda.sh
conda activate /path/to/.env
# è€Œä¸æ˜¯ï¼š
conda run -p /path/to/.env command
```

---

## ç›£æ§è¨“ç·´

### æª¢æŸ¥è¨“ç·´é€²ç¨‹

```bash
# æŸ¥çœ‹é€²ç¨‹
ps aux | grep train_dreambooth_lora_single_background | grep -v grep

# æŸ¥çœ‹ GPU ä½¿ç”¨
nvidia-smi

# æŒçºŒç›£æ§ GPU
watch -n 1 nvidia-smi
```

### æŸ¥çœ‹è¨“ç·´æ—¥èªŒ

```bash
# å¯¦æ™‚æŸ¥çœ‹æ—¥èªŒ
tail -f nohup_train_toothbrush_defective.log

# æŸ¥çœ‹æœ€æ–° 30 è¡Œ
tail -n 30 nohup_train_toothbrush_defective.log

# æŸ¥çœ‹æœ€è¿‘çš„ loss å€¼
grep "loss=" nohup_train_toothbrush_defective.log | tail -20

# æŸ¥çœ‹ç•¶å‰è¨“ç·´æ­¥æ•¸
grep "Steps:" nohup_train_toothbrush_defective.log | tail -1
```

### è¨“ç·´é€²åº¦ç¤ºä¾‹

```
Steps:  10%|â–ˆ         | 500/5000 [10:25<1:33:45,  1.25s/it, loss=0.0234, lr=2e-5]
```

è§£è®€ï¼š
- å®Œæˆ 500/5000 æ­¥ï¼ˆ10%ï¼‰
- å·²ç”¨æ™‚é–“ï¼š10 åˆ† 25 ç§’
- é è¨ˆå‰©é¤˜æ™‚é–“ï¼š1 å°æ™‚ 33 åˆ† 45 ç§’
- ç•¶å‰é€Ÿåº¦ï¼š1.25 ç§’/æ­¥
- ç•¶å‰ lossï¼š0.0234

### åœæ­¢è¨“ç·´

```bash
# æ‰¾åˆ°é€²ç¨‹ PID
ps aux | grep train_dreambooth_lora_single_background | grep -v grep

# ä½¿ç”¨ PID åœæ­¢
kill <PID>

# æˆ–å¼·åˆ¶åœæ­¢
kill -9 <PID>
```

---

## è¨“ç·´è¼¸å‡º

### æ¨¡å‹æ¬Šé‡

è¨“ç·´å®Œæˆå¾Œï¼ŒLoRA æ¬Šé‡ä¿å­˜åœ¨ï¼š
```
all_generate/toothbrush/defective/pytorch_lora_weights.bin
```

### æ¨ç†ä½¿ç”¨

```python
from diffusers import DiffusionPipeline

pipe = DiffusionPipeline.from_pretrained(
    "/path/to/stable-diffusion-v1-5",
    safety_checker=None
).to("cuda")

pipe.load_lora_weights('./all_generate/toothbrush/defective')

# ç”Ÿæˆåœ–åƒ...
```

---

## å¸¸è¦‹é…ç½®

### å¤š GPU è¨“ç·´

ä¿®æ”¹ `~/.cache/huggingface/accelerate/default_config.yaml`:

```yaml
distributed_type: 'MULTI_GPU'
num_processes: 2  # GPU æ•¸é‡
```

ç„¶å¾Œä½¿ç”¨ï¼š
```bash
CUDA_VISIBLE_DEVICES=0,1 accelerate launch ...
```

### èª¿æ•´é¡¯å­˜ä½¿ç”¨

å¦‚æœé¡¯å­˜ä¸è¶³ï¼Œèª¿æ•´ä»¥ä¸‹åƒæ•¸ï¼š

```bash
--train_batch_size=1  # æ¸›å° batch size
--gradient_accumulation_steps=2  # å¢åŠ ç´¯ç©æ­¥æ•¸ä»¥è£œå„Ÿ
--gradient_checkpointing  # å•Ÿç”¨æ¢¯åº¦æª¢æŸ¥é»ï¼ˆé™ä½é¡¯å­˜ä½¿ç”¨ï¼‰
```

### ä½¿ç”¨æ··åˆç²¾åº¦ï¼ˆå¦‚æœ PyTorch >= 2.2ï¼‰

ä¿®æ”¹ accelerate é…ç½®ï¼š
```yaml
mixed_precision: fp16  # æˆ– bf16ï¼ˆéœ€è¦ Ampere+ GPUï¼‰
```

---

## ç‰ˆæœ¬ä¿¡æ¯

æˆåŠŸæ¸¬è©¦çš„ç’°å¢ƒé…ç½®ï¼š

- **OS**: Linux 6.8.0-64-generic
- **Python**: 3.10
- **PyTorch**: 2.2.0+cu118
- **torchvision**: 0.17.0+cu118
- **torchaudio**: 2.2.0+cu118
- **xformers**: 0.0.24
- **accelerate**: 0.24.1
- **transformers**: 4.30.2
- **diffusers**: latest
- **CUDA**: 11.8
- **cuDNN**: 8.7.0.84

---

## åƒè€ƒè³‡æ–™

- [DualAnoDiff GitHub](https://github.com/yinyjin/DualAnoDiff)
- [MVTec AD Dataset](https://www.mvtec.com/company/research/datasets/mvtec-ad)
- [Diffusers Documentation](https://huggingface.co/docs/diffusers)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)

---

## æ•…éšœæ’é™¤æª¢æŸ¥æ¸…å–®

è¨“ç·´å‰æª¢æŸ¥ï¼š

- [ ] Python ç‰ˆæœ¬ = 3.10ï¼ˆä¸æ˜¯ 3.13ï¼‰
- [ ] PyTorch ç‰ˆæœ¬ >= 2.2.0
- [ ] xformers ç‰ˆæœ¬ = 0.0.24
- [ ] accelerate é…ç½®æ­£ç¢ºï¼ˆ`mixed_precision: 'no'`ï¼‰
- [ ] æ‰€æœ‰è·¯å¾‘æ›´æ–°ç‚ºçµ•å°è·¯å¾‘
- [ ] ä»£ç¢¼ä¿®æ”¹å·²æ‡‰ç”¨ï¼ˆtrain_dreambooth_lora_single_background.py å’Œ attention_processor.pyï¼‰
- [ ] MVTec æ•¸æ“šé›†å·²ä¸‹è¼‰ä¸¦æ”¾ç½®åœ¨æ­£ç¢ºä½ç½®
- [ ] Stable Diffusion v1.5 æ¨¡å‹å·²ä¸‹è¼‰
- [ ] GPU é¡¯å­˜ >= 16GBï¼ˆbatch_size=1ï¼‰æˆ– >= 20GBï¼ˆbatch_size=2ï¼‰

---

**æœ€å¾Œæ›´æ–°**: 2025-10-18
**ç¶­è­·è€…**: Claude Code Assistant
